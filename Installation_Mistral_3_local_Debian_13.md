<div align="center">

  <br></br>
  
  <a href="https://github.com/0xCyberLiTech">
    <img src="https://readme-typing-svg.herokuapp.com?font=JetBrains+Mono&size=50&duration=6000&pause=1000000000&color=FF0048&center=true&vCenter=true&width=1100&lines=%3EL'IA_" alt="Titre dynamique OPENVAS" />
  </a>
  
  <br></br>
  
  <h2>Laboratoire numÃ©rique pour la cybersÃ©curitÃ©, Linux & IT.</h2>

  <p align="center">
    <a href="https://0xcyberlitech.github.io/">
      <img src="https://img.shields.io/badge/Portfolio-0xCyberLiTech-181717?logo=github&style=flat-square" alt="ğŸŒ Portfolio" />
    </a>
    <a href="https://github.com/0xCyberLiTech">
      <img src="https://img.shields.io/badge/Profil-GitHub-181717?logo=github&style=flat-square" alt="ğŸ”— Profil GitHub" />
    </a>
    <a href="https://github.com/0xCyberLiTech/OpenVAS/releases/latest">
      <img src="https://img.shields.io/github/v/release/0xCyberLiTech/OpenVAS?label=version&style=flat-square&color=blue" alt="ğŸ“¦ DerniÃ¨re version" />
    </a>
    <a href="https://github.com/0xCyberLiTech/OpenVAS/blob/main/CHANGELOG.md">
      <img src="https://img.shields.io/badge/ğŸ“„%20Changelog-OpenVAS-blue?style=flat-square" alt="ğŸ“„ CHANGELOG OpenVAS" />
    </a>
    <a href="https://github.com/0xCyberLiTech?tab=repositories">
      <img src="https://img.shields.io/badge/DÃ©pÃ´ts-publics-blue?style=flat-square" alt="ğŸ“‚ DÃ©pÃ´ts publics" />
    </a>
    <a href="https://github.com/0xCyberLiTech/OpenVAS/graphs/contributors">
      <img src="https://img.shields.io/badge/ğŸ‘¥%20Contributeurs-cliquez%20ici-007ec6?style=flat-square" alt="ğŸ‘¥ Contributeurs OpenVAS" />
    </a>
  </p>

</div>

<div align="center">
  <img src="https://img.icons8.com/fluency/96/000000/cyber-security.png" alt="CyberSec" width="80"/>
</div>

<div align="center">
  <p>
    <strong>CybersÃ©curitÃ©</strong> <img src="https://img.icons8.com/color/24/000000/lock--v1.png"/> â€¢ <strong>Linux Debian</strong> <img src="https://img.icons8.com/color/24/000000/linux.png"/> â€¢ <strong>SÃ©curitÃ© informatique</strong> <img src="https://img.icons8.com/color/24/000000/shield-security.png"/>
  </p>
</div>

---

<div align="center">
  
## Ã€ propos & Objectifs.

</div>

Ce projet propose des solutions innovantes et accessibles en cybersÃ©curitÃ©, avec une approche centrÃ©e sur la simplicitÃ© dâ€™utilisation et lâ€™efficacitÃ©. Il vise Ã  accompagner les utilisateurs dans la protection de leurs donnÃ©es et systÃ¨mes, tout en favorisant lâ€™apprentissage et le partage des connaissances.

Le contenu est structurÃ©, accessible et optimisÃ© SEO pour rÃ©pondre aux besoins deâ€¯:
- ğŸ“ Ã‰tudiants : approfondir les connaissances
- ğŸ‘¨â€ğŸ’» Professionnels IT : outils et pratiques
- ğŸ–¥ï¸ Administrateurs systÃ¨me : sÃ©curiser lâ€™infrastructure
- ğŸ›¡ï¸ Experts cybersÃ©curitÃ© : ressources techniques
- ğŸš€ PassionnÃ©s du numÃ©rique : explorer les bonnes pratiques

---

> Guide complet expliquant, Ã©tape par Ã©tape, le fonctionnement, lâ€™installation et lâ€™utilisation de solutions dâ€™intelligence artificielle sur Debian 12 et Debian 13.

---

<div align="center" style="margin-bottom: 10px;">

### **Sommaire**

ğŸŸ¢ **Actif** â€“ DÃ©pÃ´t totalement accessible  
ğŸŸ  **Partiel** â€“ DÃ©pÃ´t partiellement accessible  
ğŸ”´ **Inactif** â€“ DÃ©pÃ´t inaccessible ou indisponible

</div>

---
## ğŸš€ Installation de Mistral 3 en local sous Debian 13

Mistral 3 est un modÃ¨le de langage avancÃ©, open source, optimisÃ© pour lâ€™exÃ©cution locale sur une machine Linux. Ce guide explique comment installer et commencer Ã  utiliser Mistral 3 sur Debian 13, Ã©tape par Ã©tape.

---

### ğŸ§© PrÃ©requis

- SystÃ¨me dâ€™exploitation : **Debian 13** (Ã  jour)
- Un utilisateur avec les droits `sudo`
- **Python 3.9+** (ou version recommandÃ©e)
- **Git** installÃ©
- AccÃ¨s Ã  Internet et suffisamment dâ€™espace disque (plusieurs Go selon la taille du modÃ¨le)

---

### 1ï¸âƒ£ Mise Ã  jour du systÃ¨me et installation des dÃ©pendances

```bash
sudo apt update && sudo apt upgrade
sudo apt install python3 python3-venv python3-pip git
```

---

### 2ï¸âƒ£ ğŸ“¦ CrÃ©ation dâ€™un environnement Python isolÃ©

```bash
python3 -m venv mistral3-env
source mistral3-env/bin/activate
```

---

### 3ï¸âƒ£ ğŸ”½ Installation du serveur et du modÃ¨le Mistral 3

#### Option 1 : Utiliser `llama-cpp-python` (recommandÃ© pour local)

> La mÃ©thode la plus simple pour utiliser Mistral 3 consiste Ã  passer par llama-cpp-python 
compatible avec les modÃ¨les Mistral au format GGUF.

1. **Installer llama-cpp-python :**

```bash
pip install --upgrade pip
pip install llama-cpp-python
```

2. **TÃ©lÃ©charger le modÃ¨le Mistral-3 au format GGUF :**

- Rendez-vous sur [HuggingFace - mistralai/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/mistralai)
- TÃ©lÃ©chargez un modÃ¨le GGUF (exempleâ€¯: `mistral-7b-instruct-v0.3.Q4_K_M.gguf`) dans un dossier.

3. **Lancer un prompt avec le modÃ¨le localâ€¯:**

```bash
python3 -m llama_cpp.server \
  --model ./chemin/vers/modele/mistral-7b-instruct-v0.3.Q4_K_M.gguf \
  --host 127.0.0.1 --port 8000
```

- Le serveur REST sera accessible sur `http://127.0.0.1:8000/v1/chat/completions`

---

#### Option 2 : Utiliser [LM Studio](https://lmstudio.ai/) (interface graphique multiplateforme)

- TÃ©lÃ©charge LM Studio (AppImage ou .deb) depuis le site officiel.
- Utilise LM Studio pour importer le modÃ¨le Mistral 3.
- DÃ©marre le modÃ¨le en local via lâ€™interface.

---

### 4ï¸âƒ£ ğŸ–²ï¸ Tester le modÃ¨le

Avec llama-cpp-pythonâ€¯:
```python
from llama_cpp import Llama

llm = Llama(model_path="chemin/vers/le-modele/mistral-7b-instruct-v0.3.Q4_K_M.gguf")
result = llm.create_completion("Explique le machine learning en une phrase.")
print(result)
```

---

### ğŸ› ï¸ DÃ©pannage

- **Erreur â€œout of memoryâ€ :** essaye une version du modÃ¨le en quantification plus lÃ©gÃ¨re (Q4, Q5â€¦).
- **Pas dâ€™AVX2 sur lâ€™ordinateur :** prends un binaire compilÃ© sans instructions AVX2 ou utilise des versions plus lÃ©gÃ¨res.
- **Pour GPU Nvidia :** consulte la doc`llama-cpp-python` pour installer avec CUDA.

---

### ğŸ”— Ressources utiles

- [Mistral AI - Site officiel](https://mistral.ai/)
- [HuggingFace - Page des modÃ¨les Mistral-3](https://huggingface.co/models?search=mistral)
- [Documentation llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [LM Studio - Interface locale](https://lmstudio.ai/)

---
## ğŸ¯ TÃ©lÃ©charger Mistral 3 â€“ Liens & Versions

### ğŸ“¥ Lien direct pour rÃ©cupÃ©rer un modÃ¨le Mistral 3

Les modÃ¨les Mistral sont disponibles publiquement sur HuggingFace.  
Voici un lien exploitable pour Mistral 3 (formats .gguf, adaptÃ© Ã  une utilisation locale)â€¯:

- **Page officielle HuggingFace des modÃ¨les Mistral 3 (GGUF)â€¯:**  
  ğŸ‘‰ [TheBloke/Mistral sur HuggingFace](https://huggingface.co/mistralai/)

*Astuceâ€¯: Tu peux remplacer le nom du fichier pour choisir une autre version/quantification si besoin. Le tÃ©lÃ©chargement peut se faire en cliquant ou via `wget`â€¯:*
```bash
wget "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf"
```

---

### ğŸ§© Quelle version choisirâ€¯?  
Les modÃ¨les publiÃ©s en GGUF proposent plusieurs **versions selon la "quantification"**â€¯:

| Version GGUF                    | RAM requise (approx.) | Vitesse        | QualitÃ©         | Usage conseillÃ©                   |
|----------------------------------|----------------------|----------------|-----------------|-----------------------------------|
| **Q2_K**                        | Faible               | TrÃ¨s rapide    | QualitÃ© basse   | DÃ©mo, anciens PC                  |
| **Q4_K_M** _(recommandÃ©)_        | Moyenne (~5-6 Go)    | Rapide         | Bonne           | Usage quotidien                   |
| **Q5_K_M**                      | Moyenne Ã  Ã©levÃ©e     | Moins rapide   | TrÃ¨s bonne      | PrÃ©cision accrue                  |
| **Q6_K, F16** _(float 16 bits)_ | Haute (>16 Go RAM)   | Plus lent      | Optimale        | Pour la meilleure qualitÃ©, gros PC/GPU|

**RÃ©sumÃ©**  
- Version **Q4_K_M**â€¯: Bon compromis entre taille mÃ©moire, rapiditÃ© et performance â€“ idÃ©ale pour PC personnels et serveurs modestes.
- Version **Q5, Q6, F16**â€¯: Plus gourmandes en RAM, mais meilleure fidÃ©litÃ© de gÃ©nÃ©ration de texte (proche du modÃ¨le dâ€™origine).
- **Plus le chiffre est bas, plus le modÃ¨le est lÃ©ger, mais la gÃ©nÃ©ration de texte est moins prÃ©cise.**

---

### ğŸ” Aller plus loin

- Parcours toutes les versions et tailles sur la page du modÃ¨le :  
  ğŸ‘‰ [Liste des fichiers sur HuggingFace](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/tree/main)
- VÃ©rifie si ton ordinateur dispose de la mÃ©moire RAM suffisante en fonction de la version choisie.

---

> **Astuce**â€¯: Il est possible dâ€™utiliser plusieurs versions selon ton besoinâ€¯: rapide pour le test, plus lourde pour la production ou la recherche de qualitÃ© optimale.

> Lâ€™installation est rapide et te permet dâ€™expÃ©rimenter localement tout le potentiel du modÃ¨le Mistral 3 sur Debian 13â€¯!

---

<div align="center">
  <a href="https://github.com/0xCyberLiTech" target="_blank" rel="noopener">
    <img src="https://skillicons.dev/icons?i=linux,debian,bash,docker,nginx,git,vim,python,markdown" alt="Skills" width="440">
  </a>
</div>

<div align="center">
  <b>ğŸ”’ Un guide proposÃ© par <a href="https://github.com/0xCyberLiTech">0xCyberLiTech</a> â€¢ Pour des tutoriels accessibles Ã  tous. ğŸ”’</b>
</div>



